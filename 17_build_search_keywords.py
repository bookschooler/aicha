"""
ì°»ì§‘ í¬ë¡¤ë§ìš© ê²€ìƒ‰ì–´ ì¡°í•© ìƒì„± ìŠ¤í¬ë¦½íŠ¸
===========================================
íë¦„:
  1ë‹¨ê³„ : to_map.csv TM(EPSG:5181) â†’ WGS84 ì¢Œí‘œ ë³€í™˜
  2ë‹¨ê³„ : ìƒê¶Œì½”ë“œëª…ì—ì„œ ì—­ëª… ì¶”ì¶œ â†’ ì¹´ì¹´ì˜¤ APIë¡œ ì—­ WGS84 ì¢Œí‘œ ìˆ˜ì§‘
  3ë‹¨ê³„ : ìƒê¶Œë³„ ìµœê·¼ì ‘ ì§€í•˜ì² ì—­ ë§¤í•‘ (scipy KDTree)
  4ë‹¨ê³„ : ìƒê¶Œì½”ë“œëª…ì—ì„œ ëœë“œë§ˆí¬ ì¶”ì¶œ
  5ë‹¨ê³„ : ê²€ìƒ‰ì–´ ì¡°í•© ìƒì„± (í–‰ì •ë™ëª… + ì—­ëª… + ëœë“œë§ˆí¬ + í‚¤ì›Œë“œ)
  ì¶œë ¥  : search_keywords.csv
"""

import pandas as pd
import numpy as np
import requests
import time
import re
import os
from pyproj import Transformer
from scipy.spatial import cKDTree
from dotenv import load_dotenv

load_dotenv()   # .env íŒŒì¼ ìë™ ë¡œë“œ

# =====================================================
# ì„¤ì •
# =====================================================
KAKAO_API_KEY = os.environ["KAKAO_API_KEY"]
DATA_PATH     = os.path.dirname(os.path.abspath(__file__))
TO_MAP_PATH   = os.path.join(DATA_PATH, "to_map.csv")
OUTPUT_PATH   = os.path.join(DATA_PATH, "search_keywords.csv")
STATION_CACHE = os.path.join(DATA_PATH, "station_coords.csv")   # ì¤‘ê°„ ì €ì¥

TEA_KEYWORDS = ["ì°»ì§‘", "í‹°ë£¸", "í‹°í•˜ìš°ìŠ¤", "í‹°ì¹´í˜", "ë‹¤ì›"]

# =====================================================
# 1ë‹¨ê³„: to_map.csv TM â†’ WGS84 ì¢Œí‘œ ë³€í™˜
# =====================================================
def convert_tm_to_wgs84(df: pd.DataFrame) -> pd.DataFrame:
    """
    to_map.csvì˜ ì—‘ìŠ¤ì¢Œí‘œ_ê°’, ì™€ì´ì¢Œí‘œ_ê°’ (EPSG:5181 TM)
    â†’ ê²½ë„(lon), ìœ„ë„(lat) (WGS84) ë¡œ ë³€í™˜
    """
    print("ğŸ“ 1ë‹¨ê³„: TM â†’ WGS84 ì¢Œí‘œ ë³€í™˜ ì¤‘...")
    transformer = Transformer.from_crs("EPSG:5181", "EPSG:4326", always_xy=True)
    lons, lats = transformer.transform(
        df["ì—‘ìŠ¤ì¢Œí‘œ_ê°’"].values,
        df["ì™€ì´ì¢Œí‘œ_ê°’"].values
    )
    df = df.copy()
    df["lon"] = lons
    df["lat"] = lats
    print(f"   âœ… {len(df)}ê°œ ìƒê¶Œ ë³€í™˜ ì™„ë£Œ")
    return df


# =====================================================
# 2ë‹¨ê³„: ì—­ ì¢Œí‘œ ìˆ˜ì§‘ (ì¹´ì¹´ì˜¤ í‚¤ì›Œë“œ ê²€ìƒ‰ API)
# =====================================================
def extract_station_names(df: pd.DataFrame) -> list[str]:
    """ìƒê¶Œì½”ë“œëª…ì—ì„œ ìœ ë‹ˆí¬ ì—­ëª… ì¶”ì¶œ"""
    station_set = set()
    for name in df["ìƒê¶Œ_ì½”ë“œ_ëª…"]:
        if "ì—­" in name:
            match = re.match(r"(.+ì—­)\s*\d*ë²ˆ?$", name.strip())
            if match:
                station_set.add(match.group(1))
    return sorted(station_set)


def get_station_coords_kakao(station_names: list[str], api_key: str) -> pd.DataFrame:
    """
    ì¹´ì¹´ì˜¤ í‚¤ì›Œë“œ ê²€ìƒ‰ APIë¡œ ì—­ëª… â†’ WGS84 ì¢Œí‘œ ìˆ˜ì§‘
    ìºì‹œ íŒŒì¼ì´ ìˆìœ¼ë©´ ì¬ì‚¬ìš© (API ì ˆì•½)
    """
    # ìºì‹œ ìˆìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
    if os.path.exists(STATION_CACHE):
        cached = pd.read_csv(STATION_CACHE, encoding="utf-8-sig")
        print(f"   ğŸ“¦ ìºì‹œ ë¡œë“œ: {len(cached)}ê°œ ì—­ ì¢Œí‘œ (ì¬ìˆ˜ì§‘ ìƒëµ)")
        return cached

    print(f"ğŸš‡ 2ë‹¨ê³„: {len(station_names)}ê°œ ì—­ ì¢Œí‘œ ìˆ˜ì§‘ ì¤‘...")
    headers = {"Authorization": f"KakaoAK {api_key}"}
    url     = "https://dapi.kakao.com/v2/local/search/keyword.json"

    rows = []
    for i, station in enumerate(station_names, 1):
        params = {"query": station, "size": 1, "category_group_code": "SW8"}  # SW8 = ì§€í•˜ì² ì—­
        try:
            resp = requests.get(url, headers=headers, params=params, timeout=5)
            data = resp.json()
            docs = data.get("documents", [])
            if docs:
                rows.append({
                    "ì—­ëª…": station,
                    "ì—­_lon": float(docs[0]["x"]),
                    "ì—­_lat": float(docs[0]["y"]),
                })
            else:
                # SW8 ì¹´í…Œê³ ë¦¬ë¡œ ëª» ì°¾ìœ¼ë©´ ì¹´í…Œê³ ë¦¬ ì—†ì´ ì¬ì‹œë„
                params2 = {"query": station, "size": 1}
                resp2   = requests.get(url, headers=headers, params=params2, timeout=5)
                docs2   = resp2.json().get("documents", [])
                if docs2:
                    rows.append({
                        "ì—­ëª…": station,
                        "ì—­_lon": float(docs2[0]["x"]),
                        "ì—­_lat": float(docs2[0]["y"]),
                    })
                else:
                    print(f"   âš ï¸  ì¢Œí‘œ ì—†ìŒ: {station}")
        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜ ({station}): {e}")

        if i % 50 == 0:
            print(f"   {i}/{len(station_names)} ì™„ë£Œ...")
        time.sleep(0.2)   # API rate limit ì¤€ìˆ˜

    df_stations = pd.DataFrame(rows)
    df_stations.to_csv(STATION_CACHE, index=False, encoding="utf-8-sig")
    print(f"   âœ… {len(df_stations)}ê°œ ì—­ ì¢Œí‘œ ìˆ˜ì§‘ ì™„ë£Œ â†’ {STATION_CACHE}")
    return df_stations


# =====================================================
# 3ë‹¨ê³„: ìƒê¶Œë³„ ìµœê·¼ì ‘ ì§€í•˜ì² ì—­ ë§¤í•‘ (KDTree)
# =====================================================
def map_nearest_station(df_map: pd.DataFrame, df_stations: pd.DataFrame) -> pd.DataFrame:
    """
    ê° ìƒê¶Œì˜ WGS84 ì¢Œí‘œì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ì§€í•˜ì² ì—­ì„ ì°¾ì•„
    'ìµœê·¼ì ‘_ì—­ëª…', 'ìµœê·¼ì ‘_ì—­_ê±°ë¦¬m' ì»¬ëŸ¼ ì¶”ê°€
    """
    print("ğŸ” 3ë‹¨ê³„: ìµœê·¼ì ‘ ì§€í•˜ì² ì—­ ë§¤í•‘ ì¤‘ (KDTree)...")

    # KDTreeìš© ì¢Œí‘œ ë°°ì—´ (ë‹¨ìœ„: ë„ â†’ ê±°ë¦¬ ê·¼ì‚¬ë¥¼ ìœ„í•´ ìœ„ë„ ê¸°ì¤€ ìŠ¤ì¼€ì¼ë§)
    # ì„œìš¸ ìœ„ë„ ~37.5Â° ê¸°ì¤€: ê²½ë„ 1ë„ â‰ˆ 88.8km, ìœ„ë„ 1ë„ â‰ˆ 111km
    LAT_REF = 37.5
    LON_SCALE = np.cos(np.radians(LAT_REF))   # ê²½ë„ ë³´ì • ê³„ìˆ˜

    station_coords = df_stations[["ì—­_lon", "ì—­_lat"]].values.copy()
    station_coords[:, 0] *= LON_SCALE          # ê²½ë„ ìŠ¤ì¼€ì¼ ë³´ì •

    map_coords = df_map[["lon", "lat"]].values.copy()
    map_coords[:, 0] *= LON_SCALE

    tree = cKDTree(station_coords)
    dists, idxs = tree.query(map_coords, k=1)

    # ë„ ë‹¨ìœ„ ê±°ë¦¬ â†’ ë¯¸í„° ë³€í™˜ (ìœ„ë„ 1ë„ â‰ˆ 111,320m)
    dists_m = dists * 111320

    df_result = df_map.copy()
    df_result["ìµœê·¼ì ‘_ì—­ëª…"]    = df_stations["ì—­ëª…"].values[idxs]
    df_result["ìµœê·¼ì ‘_ì—­_ê±°ë¦¬m"] = np.round(dists_m).astype(int)

    # ê±°ë¦¬ ë¶„í¬ í™•ì¸
    print(f"   ê±°ë¦¬ ë¶„í¬: ì¤‘ì•™ê°’={np.median(dists_m):.0f}m  "
          f"75%={np.percentile(dists_m,75):.0f}m  "
          f"ìµœëŒ€={dists_m.max():.0f}m")
    far_count = (dists_m > 1000).sum()
    print(f"   âš ï¸  1km ì´ˆê³¼ ìƒê¶Œ: {far_count}ê°œ (ê²€ìƒ‰ì–´ í’ˆì§ˆ ì£¼ì˜)")
    print(f"   âœ… ìµœê·¼ì ‘ ì—­ ë§¤í•‘ ì™„ë£Œ")
    return df_result


# =====================================================
# 4ë‹¨ê³„: ìƒê¶Œì½”ë“œëª…ì—ì„œ ëœë“œë§ˆí¬ ì¶”ì¶œ
# =====================================================

# í¬í•¨ ì‹œ ìœ íš¨ ëœë“œë§ˆí¬ë¡œ ì¸ì • (ì°»ì§‘ ê²€ìƒ‰ ë§¥ë½ì—ì„œ ì˜ë¯¸ ìˆëŠ” ì¥ì†Œ)
LANDMARK_INCLUDE = [
    "ë‹¨ê¸¸", "ê±°ë¦¬", "ë§ˆì„", "ê¸¸", "ì‹œì¥", "ê´‘ì¥", "ê³µì›",
    "ê¶", "ì„±ê³½", "ë¯¸ìˆ ê´€", "ë°•ë¬¼ê´€", "ê¸°ë…ê´€", "ìˆ˜ëª©ì›",
    "ìƒíƒœ", "í˜¸ìˆ˜", "ìˆ²", "í„°ë¯¸ë„", "ëŒ€í•™êµ", "ìº í¼ìŠ¤",
]

# í¬í•¨ ì‹œ ì œì™¸ (ê²€ìƒ‰ì–´ë¡œ ì˜ë¯¸ ì—†ëŠ” ì‹œì„¤/ê¸°ê´€)
LANDMARK_EXCLUDE = [
    "ì´ˆë“±í•™êµ", "ì¤‘í•™êµ", "ê³ ë“±í•™êµ", "ì–´ë¦°ì´ê³µì›",
    "ì•„íŒŒíŠ¸", "ë§¨ì…˜", "ë˜ë¯¸ì•ˆ", "íìŠ¤í…Œì´íŠ¸", "ìì´", "í‘¸ë¥´ì§€ì˜¤",
    "ì£¼ë¯¼ì„¼í„°", "ì²´ìœ¡ì„¼í„°", "êµ¬ì²­", "ë™ì‚¬ë¬´ì†Œ", "íŒŒì¶œì†Œ", "ì§€êµ¬ëŒ€",
    "ê²½ì°°ì„œ", "ì†Œë°©ì„œ", "ì„¸ë¬´ì„œ", "ì€í–‰", "ì£¼ì°¨ì¥", "ìš°ì²´êµ­",
    "ì˜†", "ì•ê¸¸", "ë¶€ê·¼", "ê·¼ì²˜",
]


def extract_landmarks(df: pd.DataFrame) -> dict[int, list[str]]:
    """
    ìƒê¶Œì½”ë“œëª…ì—ì„œ ëœë“œë§ˆí¬ í›„ë³´ ì¶”ì¶œ
    ë°˜í™˜: {ìƒê¶Œ_ì½”ë“œ: [ëœë“œë§ˆí¬ëª…, ...]}

    ì²˜ë¦¬ ë¡œì§:
      1) ê´„í˜¸ ì•ˆ ë‚´ìš© ë¶„ë¦¬ â†’ ë©”ì¸ëª… + ê´„í˜¸ë‚´ìš© ê°ê° í›„ë³´
      2) ì—­ëª…/ë‹¨ìˆœ ìœ„ì¹˜ëª…("ì˜†","ì•") ì œì™¸
      3) INCLUDE í‚¤ì›Œë“œ í¬í•¨ AND EXCLUDE í‚¤ì›Œë“œ ë¯¸í¬í•¨ì¸ ê²ƒë§Œ ì±„íƒ
    """
    result: dict[int, list[str]] = {}

    for _, row in df.iterrows():
        code = row["ìƒê¶Œ_ì½”ë“œ"]
        name = str(row["ìƒê¶Œ_ì½”ë“œ_ëª…"]).strip()

        # ì´ë¯¸ ì—­ëª…ìœ¼ë¡œ ì²˜ë¦¬ë˜ëŠ” ìƒê¶Œì€ ê±´ë„ˆëœ€
        if re.match(r".+ì—­\s*\d*ë²ˆ?$", name):
            continue

        # ê´„í˜¸ ë¶„ë¦¬ â†’ [ë©”ì¸ëª…, ê´„í˜¸ë‚´ìš©1, ê´„í˜¸ë‚´ìš©2, ...]
        brackets = re.findall(r"[(\(](.+?)[)\)]", name)
        main     = re.sub(r"[(\(].+?[)\)]", "", name).strip()
        candidates = [main] + brackets

        landmarks = []
        for cand in candidates:
            cand = cand.strip()
            if not cand:
                continue
            # ì œì™¸ í‚¤ì›Œë“œ ì²´í¬
            if any(ex in cand for ex in LANDMARK_EXCLUDE):
                continue
            # í¬í•¨ í‚¤ì›Œë“œ ì²´í¬
            if any(inc in cand for inc in LANDMARK_INCLUDE):
                landmarks.append(cand)

        if landmarks:
            result[code] = landmarks

    return result


# =====================================================
# 5ë‹¨ê³„: ê²€ìƒ‰ì–´ ì¡°í•© ìƒì„±
# =====================================================
def build_search_queries(df: pd.DataFrame) -> pd.DataFrame:
    """
    ìƒê¶Œë³„ë¡œ ì„¸ ì¶• Ã— í‚¤ì›Œë“œ ì¡°í•©ì˜ ê²€ìƒ‰ì–´ ëª©ë¡ ìƒì„±
      ì¶• 1: í–‰ì •ë™ëª…         (ì˜ˆ: ì„±ìˆ˜ë™)
      ì¶• 2: ìµœê·¼ì ‘ ì§€í•˜ì² ì—­   (ì˜ˆ: ì„±ìˆ˜ì—­)
      ì¶• 3: ëœë“œë§ˆí¬         (ì˜ˆ: ê²½ë¦¬ë‹¨ê¸¸, í™©í•™ë™ë²¼ë£©ì‹œì¥)
    ì¤‘ë³µ ê²€ìƒ‰ì–´ëŠ” ì œê±° (ì—¬ëŸ¬ ìƒê¶Œì´ ê°™ì€ ì§€ì—­ì´ë©´ í•œ ë²ˆë§Œ)
    """
    print("ğŸ“ 5ë‹¨ê³„: ê²€ìƒ‰ì–´ ì¡°í•© ìƒì„± ì¤‘...")

    # ëœë“œë§ˆí¬ ì¶”ì¶œ
    landmark_map = extract_landmarks(df)
    total_landmarks = sum(len(v) for v in landmark_map.values())
    print(f"   ëœë“œë§ˆí¬ ì¶”ì¶œ: {len(landmark_map)}ê°œ ìƒê¶Œ, ì´ {total_landmarks}ê°œ í›„ë³´")

    rows = []
    seen = set()   # ì¤‘ë³µ ì œê±°ìš©

    for _, r in df.iterrows():
        dong    = str(r["í–‰ì •ë™_ì½”ë“œ_ëª…"]).strip()
        station = str(r["ìµœê·¼ì ‘_ì—­ëª…"]).strip()
        code    = r["ìƒê¶Œ_ì½”ë“œ"]
        landmarks = landmark_map.get(code, [])

        for kw in TEA_KEYWORDS:
            # ì¶• 1: í–‰ì •ë™ ê¸°ë°˜
            q_dong = f"{dong} {kw}"
            if q_dong not in seen:
                rows.append({
                    "ê²€ìƒ‰ì–´"      : q_dong,
                    "ê²€ìƒ‰ì–´_ìœ í˜•" : "í–‰ì •ë™",
                    "ê¸°ì¤€_ì§€ì—­"   : dong,
                    "í‚¤ì›Œë“œ"      : kw,
                    "ëŒ€í‘œ_ìƒê¶Œì½”ë“œ": code,
                })
                seen.add(q_dong)

            # ì¶• 2: ì—­ ê¸°ë°˜
            q_station = f"{station} {kw}"
            if q_station not in seen:
                rows.append({
                    "ê²€ìƒ‰ì–´"      : q_station,
                    "ê²€ìƒ‰ì–´_ìœ í˜•" : "ì§€í•˜ì² ì—­",
                    "ê¸°ì¤€_ì§€ì—­"   : station,
                    "í‚¤ì›Œë“œ"      : kw,
                    "ëŒ€í‘œ_ìƒê¶Œì½”ë“œ": code,
                })
                seen.add(q_station)

            # ì¶• 3: ëœë“œë§ˆí¬ ê¸°ë°˜
            for lm in landmarks:
                q_lm = f"{lm} {kw}"
                if q_lm not in seen:
                    rows.append({
                        "ê²€ìƒ‰ì–´"      : q_lm,
                        "ê²€ìƒ‰ì–´_ìœ í˜•" : "ëœë“œë§ˆí¬",
                        "ê¸°ì¤€_ì§€ì—­"   : lm,
                        "í‚¤ì›Œë“œ"      : kw,
                        "ëŒ€í‘œ_ìƒê¶Œì½”ë“œ": code,
                    })
                    seen.add(q_lm)

    df_queries = pd.DataFrame(rows)
    print(f"   í–‰ì •ë™ ê¸°ë°˜  : {(df_queries['ê²€ìƒ‰ì–´_ìœ í˜•']=='í–‰ì •ë™').sum()}ê°œ")
    print(f"   ì§€í•˜ì² ì—­ ê¸°ë°˜: {(df_queries['ê²€ìƒ‰ì–´_ìœ í˜•']=='ì§€í•˜ì² ì—­').sum()}ê°œ")
    print(f"   ëœë“œë§ˆí¬ ê¸°ë°˜: {(df_queries['ê²€ìƒ‰ì–´_ìœ í˜•']=='ëœë“œë§ˆí¬').sum()}ê°œ")
    print(f"   ì´ ê²€ìƒ‰ì–´ ìˆ˜ : {len(df_queries)}ê°œ (ì¤‘ë³µ ì œê±° ì™„ë£Œ)")
    return df_queries


# =====================================================
# ìµœê·¼ì ‘ ì—­ ë§¤í•‘ ê²°ê³¼ë¥¼ to_mapì— í•©ì³ì„œ ì €ì¥
# =====================================================
def save_map_with_station(df: pd.DataFrame):
    out = os.path.join(DATA_PATH, "to_map_with_station.csv")
    cols = ["ìƒê¶Œ_ì½”ë“œ", "ìƒê¶Œ_ì½”ë“œ_ëª…", "í–‰ì •ë™_ì½”ë“œ_ëª…", "ìì¹˜êµ¬_ì½”ë“œ_ëª…",
            "lon", "lat", "ìµœê·¼ì ‘_ì—­ëª…", "ìµœê·¼ì ‘_ì—­_ê±°ë¦¬m"]
    df[cols].to_csv(out, index=False, encoding="utf-8-sig")
    print(f"   ğŸ’¾ ìƒê¶Œ+ì—­ ë§¤í•‘ ì €ì¥ â†’ {out}")


# =====================================================
# ì‹¤í–‰
# =====================================================
if __name__ == "__main__":
    print("=" * 55)
    print("  ì°»ì§‘ í¬ë¡¤ë§ ê²€ìƒ‰ì–´ ì¡°í•© ìƒì„±ê¸°")
    print("=" * 55)

    # ë°ì´í„° ë¡œë“œ
    df_map = pd.read_csv(TO_MAP_PATH, encoding="utf-8-sig")
    print(f"ğŸ“‚ to_map.csv ë¡œë“œ: {len(df_map)}ê°œ ìƒê¶Œ\n")

    # 1ë‹¨ê³„: ì¢Œí‘œ ë³€í™˜
    df_map = convert_tm_to_wgs84(df_map)

    # 2ë‹¨ê³„: ì—­ ì¢Œí‘œ ìˆ˜ì§‘
    station_names = extract_station_names(df_map)
    print(f"   ì¶”ì¶œëœ ìœ ë‹ˆí¬ ì—­ëª…: {len(station_names)}ê°œ")
    df_stations = get_station_coords_kakao(station_names, KAKAO_API_KEY)

    # 3ë‹¨ê³„: ìµœê·¼ì ‘ ì—­ ë§¤í•‘
    df_map = map_nearest_station(df_map, df_stations)
    save_map_with_station(df_map)

    # 4~5ë‹¨ê³„: ëœë“œë§ˆí¬ ì¶”ì¶œ + ê²€ìƒ‰ì–´ ì¡°í•© ìƒì„±
    df_queries = build_search_queries(df_map)
    df_queries.to_csv(OUTPUT_PATH, index=False, encoding="utf-8-sig")

    print(f"\nâœ… ì™„ë£Œ! ê²€ìƒ‰ì–´ ëª©ë¡ ì €ì¥ â†’ {OUTPUT_PATH}")
    print("\n[ ë¯¸ë¦¬ë³´ê¸° ]")
    print(df_queries.head(10).to_string(index=False))
    print("\n[ í‚¤ì›Œë“œ ìœ í˜•ë³„ ìš”ì•½ ]")
    print(df_queries.groupby(["ê²€ìƒ‰ì–´_ìœ í˜•", "í‚¤ì›Œë“œ"]).size().unstack(fill_value=0))
